import aiohttp
import asyncio
import logging
import re
import textwrap
import socket
from bs4 import BeautifulSoup
from collections import defaultdict
from playwright.async_api import (
    async_playwright,
    TimeoutError as PlaywrightTimeoutError,
)
from pprint import pprint

logger = logging.getLogger(__name__)


async def get_customs_duty(hs_code):
    url = f"https://www.alta.ru/tnved/code/{hs_code}/"

    async with aiohttp.ClientSession() as session:
        html = await fetch_html(session, url)
        soup = BeautifulSoup(html, "html.parser")
        duty = {}

        # –ò–º–ø–æ—Ä—Ç
        import_tag = soup.select_one("fieldset.pTnved_customs b.black")
        duty["import"] = import_tag.text.strip() if import_tag else "–Ω–µ —É–∫–∞–∑–∞–Ω–æ"

        # –≠–∫—Å–ø–æ—Ä—Ç
        export_rows = soup.select("fieldset.pTnved_customs tr.pTnved_item")
        export_text = "–Ω–µ —É–∫–∞–∑–∞–Ω–æ"
        excise_text = "–Ω–µ —É–∫–∞–∑–∞–Ω–æ"

        for row in export_rows:
            label = row.select_one("td:first-child b")
            value = row.select_one("td:nth-child(2)")
            if not label or not value:
                continue

            label_text = label.text.strip().lower()
            value_text = value.text.strip()

            if "—ç–∫—Å–ø–æ—Ä—Ç" in label_text or "–ø–æ—à–ª–∏–Ω" in label_text:
                export_text = value_text
            elif "–∞–∫—Ü–∏–∑" in label_text:
                excise_text = value_text

        duty["export"] = export_text
        duty["excise"] = excise_text

        return duty if duty else "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞–≤–∫—É –ø–æ—à–ª–∏–Ω—ã."


async def fetch_official_description(session, code):
    url = f"https://www.ifcg.ru/kb/tnved/{code}"
    html = await fetch_html(session, url)
    soup = BeautifulSoup(html, "html.parser")
    tag = soup.select_one(".subtitle")
    return tag.text.strip() if tag else "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"


def extract_rows_from_soup(soup):
    result = defaultdict(set)  # code -> set of examples
    results = soup.select(".row.row-in.mt10")
    if len(results) < 5:
        return []  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –º–µ–Ω—å—à–µ 5

    for res in results[:15]:
        code_tag = res.select_one(".col-xs-12.col-md-4.col-lg-2.mt10")
        desc_tag = res.select_one(".col-xs-12.col-md-8.col-lg-10.mt10")
        if code_tag and desc_tag:
            code = code_tag.text.strip().replace("\xa0", "").replace(" ", "")
            example = desc_tag.text.strip()
            if re.fullmatch(r"\d{10}", code):  # –ø—Ä–æ–≤–µ—Ä–∫–∞: —Ä–æ–≤–Ω–æ 10 —Ü–∏—Ñ—Ä
                result[code].add(example)
    return result


async def parse_ifcg(keywords):
    url = f"https://www.ifcg.ru/kb/tnved/search/?q={keywords}&g="
    all_examples = defaultdict(set)

    async with aiohttp.ClientSession() as session:
        html = await fetch_html(session, url)
        soup = BeautifulSoup(html, "html.parser")

        # –æ–±—Ä–µ–∑–∞–µ–º –≤—Å—ë –≤—ã—à–µ result--stat
        result_stat = soup.find(id="result--stat")
        if not result_stat:
            return []

        # current = result_stat
        # while current.previous_sibling:
        #     prev = current.previous_sibling
        #     if prev:
        #         prev.extract()
        #     else:
        #         break

        # –æ—Å–Ω–æ–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        result_part = extract_rows_from_soup(soup)
        if not result_part:
            return []

        for code, descs in result_part.items():
            all_examples[code].update(descs)

        # –¥–æ–ø. —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        link_tags = soup.select(".row.row-in.mt20.tac > .btn")
        for link_tag in link_tags[:2]:
            if link_tag.has_attr("href"):
                link = "https://www.ifcg.ru" + link_tag["href"]
                html = await fetch_html(session, link)
                sub_soup = BeautifulSoup(html, "html.parser")
                result_part = extract_rows_from_soup(sub_soup)
                for code, descs in result_part.items():
                    all_examples[code].update(descs)

        # –ü–æ–ª—É—á–∞–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
        codes = list(all_examples.keys())
        tasks = [fetch_official_description(session, code) for code in codes]
        official_descriptions = await asyncio.gather(*tasks)

        # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –º–∞—Å—Å–∏–≤
        structured_data = []
        for code, official in zip(codes, official_descriptions):
            structured_data.append(
                {
                    "code": code,
                    "official": official,
                    "examples": list(all_examples[code]),
                }
            )
        pprint(structured_data)
        return structured_data


async def fetch_html(session, url, retries=3, delay=5):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ —Å–±–æ—è—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è.
    :param session: aiohttp-—Å–µ—Å—Å–∏—è
    :param url: URL-–∞–¥—Ä–µ—Å
    :param retries: –ö–æ–ª-–≤–æ –ø–æ–ø—ã—Ç–æ–∫
    :param delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    """
    for attempt in range(1, retries + 1):
        try:
            async with session.get(url) as resp:
                return await resp.text()
        except aiohttp.ClientConnectorError as e:
            logger.warning(
                f"[{attempt}] –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å..."
            )
        except aiohttp.ClientError as e:
            logger.warning(f"[{attempt}] –û—à–∏–±–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞: {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å...")
        except Exception as e:
            logger.warning(
                f"[{attempt}] –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å..."
            )

        await asyncio.sleep(delay)

    raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ {url} –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫.")


async def parse_tnved_tree(hs_code):
    url = f"https://www.alta.ru/tnved/code/{hs_code}/"
    async with aiohttp.ClientSession() as session:
        html = await fetch_html(session, url)

    soup = BeautifulSoup(html, "html.parser")

    first_tree_block = soup.select_one("ul.pTnved_position.reset")
    if not first_tree_block:
        return "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –¥–µ—Ä–µ–≤—É –¢–ù –í–≠–î."

    items = first_tree_block.select("li.pTnved_item")
    if not items:
        return "‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –¥–µ—Ä–µ–≤–µ –¢–ù –í–≠–î."

    tree_text = "üóÇÔ∏è *–î–µ—Ä–µ–≤–æ –¢–ù –í–≠–î:*\n\n"
    indent = ""

    for item in items:
        code = item.select_one("div > b").text.strip()
        description = item.select("div")[1].text.strip()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –ø–æ –¥–ª–∏–Ω–µ —Å—Ç–∏–ª—è —à–∏—Ä–∏–Ω—ã (60px = —É—Ä–æ–≤–µ–Ω—å 1, 90px = —É—Ä–æ–≤–µ–Ω—å 2 –∏ —Ç.–¥.)
        width_style = item.select_one("div").get("style")
        width_px = int(re.search(r"width:(\d+)px", width_style).group(1))
        level = (
            width_px - 60
        ) // 30  # –£—Ä–æ–≤–µ–Ω—å –æ—Ç—Å—Ç—É–ø–∞ (60px - –±–∞–∑–æ–≤—ã–π, –∫–∞–∂–¥—ã–π –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å +30px)

        indent = "    " * level
        tree_text += f"{indent}‚ñ´Ô∏è *{code}* ‚Äî {description}\n"
    return tree_text


async def fetch_tks_explanation(hs_code):
    url = f"https://www.tks.ru/db/tnved/prim_2017/c{hs_code}/"

    async with aiohttp.ClientSession() as session:
        html = await fetch_html(session, url)

    soup = BeautifulSoup(html, "html.parser")
    content = soup.select_one("#prim_issue_content")

    if not content:
        return ["–ü–æ—è—Å–Ω–µ–Ω–∏–µ –∫ –¥–∞–Ω–Ω–æ–º—É –∫–æ–¥—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."]

    result_parts = []
    temp_table_lines = []

    for child in content.children:
        if isinstance(child, str):
            continue

        # –µ—Å–ª–∏ —ç—Ç–æ "—Ç–∞–±–ª–∏—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞", —É –∫–æ—Ç–æ—Ä–æ–π –µ—Å—Ç—å <td>
        if child.name == "table" or child.find("td"):
            rows = child.select("tr")
            for i, row in enumerate(rows):
                cells = [
                    cell.get_text(strip=True, separator=" ")
                    for cell in row.select("td")
                ]

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                if not any(cells):
                    continue

                # –ü–µ—Ä–≤—ã–π —Ä—è–¥ ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
                if i == 0 and len(cells) >= 2:
                    code = cells[0]
                    desc = cells[1]
                    temp_table_lines.append(f"üìò {code}\n{desc}")

                # –û–±—ã—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏: –ø–æ–¥–ø–æ–∑–∏—Ü–∏–∏
                elif len(cells) == 2:
                    temp_table_lines.append(f"‚Ä¢ {cells[0]} {cells[1]}")
                elif len(cells) == 3:
                    temp_table_lines.append(f"‚Ä¢ {cells[1]} {cells[2]}")
                elif len(cells) == 1:
                    temp_table_lines.append(f"{cells[0]}")

        # –ü–∞—Ä–∞–≥—Ä–∞—Ñ
        elif child.name == "p":
            text = child.get_text(strip=True, separator=" ")
            if text:
                # –µ—Å–ª–∏ –±—ã–ª–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω—ã —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã ‚Äî –¥–æ–±–∞–≤–∏–º –∏—Ö –±–ª–æ–∫–æ–º
                if temp_table_lines:
                    result_parts.append("\n".join(temp_table_lines))
                    temp_table_lines = []
                result_parts.append(text)

    # –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –±—ã–ª–∞ –≤ –∫–æ–Ω—Ü–µ
    if temp_table_lines:
        result_parts.append("\n".join(temp_table_lines))

    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å—ë –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ª–∏–º–∏—Ç—É
    full_text = "\n\n".join(result_parts)
    pages = textwrap.wrap(
        full_text, width=4000, break_long_words=False, replace_whitespace=False
    )

    return pages or ["–ü–æ—è—Å–Ω–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"]


async def fetch_examples(hs_code):
    url = f"https://www.ifcg.ru/kb/tnved/{hs_code}/"
    async with aiohttp.ClientSession() as session:
        html = await fetch_html(session, url)

    soup = BeautifulSoup(html, "html.parser")

    examples = []
    for sample in soup.select(".row.row-in.tnv-samples"):
        example_text = sample.select_one(".col-md-8")
        if example_text:
            text = example_text.get_text(strip=True).replace("\xa0", " ")
            examples.append(f"‚Ä¢ {text}")

    if not examples:
        return ["–ü—Ä–∏–º–µ—Ä—ã –¥–µ–∫–ª–∞—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç"]

    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤, —Å–∫–æ–ª—å–∫–æ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –æ–¥–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ Telegram
    current_text = "üìù <b>–ü—Ä–∏–º–µ—Ä—ã –¥–µ–∫–ª–∞—Ä–∏—Ä–æ–≤–∞–Ω–∏—è:</b>\n\n"
    for example in examples:
        if len(current_text) + len(example) + 2 > 4000:
            break
        current_text += example + "\n\n"

    return current_text.strip()


async def parse_tks_info(hs_code: str):
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=True)
        page = await browser.new_page()

        await page.goto("https://www.tks.ru/db/tnved/tree/")

        # –í–≤–æ–¥–∏–º –∫–æ–¥ –≤ –ø–æ–ª–µ
        await page.fill("#tnved-search__input", hs_code)

        # –ù–∞–∂–∏–º–∞–µ–º –∫–Ω–æ–ø–∫—É –ø–æ–∏—Å–∫–∞
        await page.click("#tnved-search__submit")

        # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        await page.wait_for_selector(".tree-list__code", timeout=10000)

        # –ö–ª–∏–∫–∞–µ–º –Ω–∞ –Ω—É–∂–Ω—ã–π –∫–æ–¥
        await page.click(f".tree-list__code:text('{hs_code}')")

        # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –æ–∫–Ω–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        await page.wait_for_selector("#code_info")
        content = await page.content()

        await browser.close()

        soup = BeautifulSoup(content, "html.parser")

        # –ü–∞—Ä—Å–∏–º —Å–µ–∫—Ü–∏–∏ –ò–º–ø–æ—Ä—Ç –∏ –≠–∫—Å–ø–æ—Ä—Ç
        info_sections = soup.select("#code_info .product-info__section")
        result = ""

        for section in info_sections:
            section_title = section.select_one(
                ".product-info__section-title"
            ).text.strip()
            result += f"\n*{section_title}*\n\n"

            for row in section.select("table.product-info__table tr"):
                cols = row.find_all("td")
                if len(cols) >= 2:
                    label = cols[0].text.strip()
                    value = cols[1].text.strip()
                    result += f"*{label}* {value}\n"

        return result


async def parse_tks_info2(hs_code: str) -> str:
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ DNS ‚Äî —á—Ç–æ–±—ã –Ω–µ –∂–¥–∞—Ç—å —Ç–∞–π–º–∞—É—Ç Playwright, –µ—Å–ª–∏ –¥–æ–º–µ–Ω –Ω–µ —Ä–µ–∑–æ–ª–≤–∏—Ç—Å—è
    try:
        socket.gethostbyname("www.tks.ru")
    except socket.gaierror:
        return "‚ö†Ô∏è –°–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

    url = "https://www.tks.ru/db/tnved/tree/"
    max_retries = 3
    backoff = 1.0  # —Å–µ–∫—É–Ω–¥—ã

    for attempt in range(1, max_retries + 1):
        try:
            async with async_playwright() as pw:
                browser = await pw.chromium.launch(headless=True)
                page = await browser.new_page()
                # 2. –ù–∞–≤–∏–≥–∞—Ü–∏—è —Å –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–º –æ–∂–∏–¥–∞–Ω–∏–µ–º —Å–æ–±—ã—Ç–∏–π
                await page.goto(url, wait_until="domcontentloaded", timeout=15_000)

                # 3. –í–≤–æ–¥–∏–º –∫–æ–¥ –∏ –∏—â–µ–º
                await page.fill("#tnved-search__input", hs_code)
                await page.click("#tnved-search__submit")

                # 4. –ñ–¥—ë–º –ø–æ—è–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                await page.wait_for_selector(".tree-list__code", timeout=10_000)
                await page.click(f".tree-list__code:text('{hs_code}')")
                await page.wait_for_selector("#code_info", timeout=10_000)

                # 5. –°—á–∏—Ç—ã–≤–∞–µ–º HTML –∏ –∑–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä
                content = await page.content()
                await browser.close()

            # 6. –ü–∞—Ä—Å–∏–º BeautifulSoup‚Äô–æ–º
            soup = BeautifulSoup(content, "html.parser")
            info_sections = soup.select("#code_info .product-info__section")
            if not info_sections:
                return "‚ÑπÔ∏è –ü–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∫–æ–¥—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."

            result = ""
            for section in info_sections:
                title = section.select_one(".product-info__section-title").text.strip()
                result += f"\n*{title}*\n"
                for row in section.select("table.product-info__table tr"):
                    cols = row.find_all("td")
                    if len(cols) >= 2:
                        label = cols[0].text.strip()
                        value = cols[1].text.strip()
                        result += f"‚Ä¢ *{label}*: {value}\n"
            return result

        except PlaywrightTimeoutError:
            # 7. –¢–∞–π–º–∞—É—Ç—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
            if attempt == max_retries:
                return "‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞ –Ω–µ–º–Ω–æ–≥–æ –ø–æ–∑–∂–µ."
            await asyncio.sleep(backoff)
            backoff *= 2

        except Exception as e:
            # 8. –õ–æ–≥–∏—Ä—É–µ–º –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É
            if attempt == max_retries:
                return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: {e}"
            await asyncio.sleep(backoff)
            backoff *= 2

    # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
    return "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."


# asyncio.run(parse_ifcg("–®–∏–Ω—ã+–õ–µ–≥–∫–æ–≤—ã–µ+–†–µ–∑–∏–Ω–∞"))
