import aiohttp
import asyncio
import logging
import re
import textwrap
import socket
import random
from bs4 import BeautifulSoup
from collections import defaultdict
from playwright.async_api import (
    async_playwright,
    TimeoutError as PlaywrightTimeoutError,
)
from pprint import pprint

logger = logging.getLogger(__name__)


async def get_customs_duty(hs_code):
    url = f"https://www.alta.ru/tnved/code/{hs_code}/"

    async with aiohttp.ClientSession() as session:
        html = await fetch_html(session, url)
        soup = BeautifulSoup(html, "html.parser")
        duty = {}

        # –ò–º–ø–æ—Ä—Ç
        import_tag = soup.select_one("fieldset.pTnved_customs b.black")
        duty["import"] = import_tag.text.strip() if import_tag else "–Ω–µ —É–∫–∞–∑–∞–Ω–æ"

        # –≠–∫—Å–ø–æ—Ä—Ç
        export_rows = soup.select("fieldset.pTnved_customs tr.pTnved_item")
        export_text = "–Ω–µ —É–∫–∞–∑–∞–Ω–æ"
        excise_text = "–Ω–µ —É–∫–∞–∑–∞–Ω–æ"

        for row in export_rows:
            label = row.select_one("td:first-child b")
            value = row.select_one("td:nth-child(2)")
            if not label or not value:
                continue

            label_text = label.text.strip().lower()
            value_text = value.text.strip()

            if "—ç–∫—Å–ø–æ—Ä—Ç" in label_text or "–ø–æ—à–ª–∏–Ω" in label_text:
                export_text = value_text
            elif "–∞–∫—Ü–∏–∑" in label_text:
                excise_text = value_text

        duty["export"] = export_text
        duty["excise"] = excise_text

        return duty if duty else "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞–≤–∫—É –ø–æ—à–ª–∏–Ω—ã."


async def fetch_official_description(session, code):
    url = f"https://www.ifcg.ru/kb/tnved/{code}"
    html = await fetch_html(session, url)
    soup = BeautifulSoup(html, "html.parser")
    tag = soup.select_one(".subtitle")
    return tag.text.strip() if tag else "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"


def extract_rows_from_soup(soup):
    result = defaultdict(set)  # code -> set of examples
    results = soup.select(".row.row-in.mt10")
    if len(results) < 5:
        return []  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –º–µ–Ω—å—à–µ 5

    for res in results[:15]:
        code_tag = res.select_one(".col-xs-12.col-md-4.col-lg-2.mt10")
        desc_tag = res.select_one(".col-xs-12.col-md-8.col-lg-10.mt10")
        if code_tag and desc_tag:
            code = code_tag.text.strip().replace("\xa0", "").replace(" ", "")
            example = desc_tag.text.strip()
            if re.fullmatch(r"\d{10}", code):  # –ø—Ä–æ–≤–µ—Ä–∫–∞: —Ä–æ–≤–Ω–æ 10 —Ü–∏—Ñ—Ä
                result[code].add(example)
    return result


async def parse_ifcg(keywords):
    url = f"https://www.ifcg.ru/kb/tnved/search/?q={keywords}&g="
    all_examples = defaultdict(set)

    async with aiohttp.ClientSession() as session:
        html = await fetch_html(session, url)
        soup = BeautifulSoup(html, "html.parser")

        # –æ–±—Ä–µ–∑–∞–µ–º –≤—Å—ë –≤—ã—à–µ result--stat
        result_stat = soup.find(id="result--stat")
        if not result_stat:
            return []

        # current = result_stat
        # while current.previous_sibling:
        #     prev = current.previous_sibling
        #     if prev:
        #         prev.extract()
        #     else:
        #         break

        # –æ—Å–Ω–æ–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        result_part = extract_rows_from_soup(soup)
        if not result_part:
            return []

        for code, descs in result_part.items():
            all_examples[code].update(descs)

        # –¥–æ–ø. —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        link_tags = soup.select(".row.row-in.mt20.tac > .btn")
        for link_tag in link_tags[:2]:
            if link_tag.has_attr("href"):
                link = "https://www.ifcg.ru" + link_tag["href"]
                html = await fetch_html(session, link)
                sub_soup = BeautifulSoup(html, "html.parser")
                result_part = extract_rows_from_soup(sub_soup)
                for code, descs in result_part.items():
                    all_examples[code].update(descs)

        # –ü–æ–ª—É—á–∞–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
        codes = list(all_examples.keys())
        tasks = [fetch_official_description(session, code) for code in codes]
        official_descriptions = await asyncio.gather(*tasks)

        # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –º–∞—Å—Å–∏–≤
        structured_data = []
        for code, official in zip(codes, official_descriptions):
            structured_data.append(
                {
                    "code": code,
                    "official": official,
                    "examples": list(all_examples[code]),
                }
            )
        # pprint(structured_data)
        return structured_data


async def fetch_html(session, url, retries=3, delay=5):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ —Å–±–æ—è—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è.
    :param session: aiohttp-—Å–µ—Å—Å–∏—è
    :param url: URL-–∞–¥—Ä–µ—Å
    :param retries: –ö–æ–ª-–≤–æ –ø–æ–ø—ã—Ç–æ–∫
    :param delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    """
    for attempt in range(1, retries + 1):
        try:
            async with session.get(url) as resp:
                return await resp.text()
        except aiohttp.ClientConnectorError as e:
            logger.warning(
                f"[{attempt}] –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å..."
            )
        except aiohttp.ClientError as e:
            logger.warning(f"[{attempt}] –û—à–∏–±–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞: {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å...")
        except Exception as e:
            logger.warning(
                f"[{attempt}] –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å..."
            )

        await asyncio.sleep(delay)

    raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ {url} –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫.")


async def parse_tnved_tree(hs_code):
    url = f"https://www.alta.ru/tnved/code/{hs_code}/"
    async with aiohttp.ClientSession() as session:
        html = await fetch_html(session, url)

    soup = BeautifulSoup(html, "html.parser")

    first_tree_block = soup.select_one("ul.pTnved_position.reset")
    if not first_tree_block:
        return "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –¥–µ—Ä–µ–≤—É –¢–ù –í–≠–î."

    items = first_tree_block.select("li.pTnved_item")
    if not items:
        return "‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –¥–µ—Ä–µ–≤–µ –¢–ù –í–≠–î."

    tree_text = "üóÇÔ∏è *–î–µ—Ä–µ–≤–æ –¢–ù –í–≠–î:*\n\n"
    indent = ""

    for item in items:
        code = item.select_one("div > b").text.strip()
        description = item.select("div")[1].text.strip()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –ø–æ –¥–ª–∏–Ω–µ —Å—Ç–∏–ª—è —à–∏—Ä–∏–Ω—ã (60px = —É—Ä–æ–≤–µ–Ω—å 1, 90px = —É—Ä–æ–≤–µ–Ω—å 2 –∏ —Ç.–¥.)
        width_style = item.select_one("div").get("style")
        width_px = int(re.search(r"width:(\d+)px", width_style).group(1))
        level = (
            width_px - 60
        ) // 30  # –£—Ä–æ–≤–µ–Ω—å –æ—Ç—Å—Ç—É–ø–∞ (60px - –±–∞–∑–æ–≤—ã–π, –∫–∞–∂–¥—ã–π –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å +30px)

        indent = "    " * level
        tree_text += f"{indent}‚ñ´Ô∏è *{code}* ‚Äî {description}\n"
    return tree_text


async def fetch_tks_explanation(hs_code):
    url = f"https://www.tks.ru/db/tnved/prim_2017/c{hs_code}/"

    async with aiohttp.ClientSession() as session:
        html = await fetch_html(session, url)

    soup = BeautifulSoup(html, "html.parser")
    content = soup.select_one("#prim_issue_content")

    if not content:
        return ["–ü–æ—è—Å–Ω–µ–Ω–∏–µ –∫ –¥–∞–Ω–Ω–æ–º—É –∫–æ–¥—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."]

    result_parts = []
    temp_table_lines = []

    for child in content.children:
        if isinstance(child, str):
            continue

        # –µ—Å–ª–∏ —ç—Ç–æ "—Ç–∞–±–ª–∏—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞", —É –∫–æ—Ç–æ—Ä–æ–π –µ—Å—Ç—å <td>
        if child.name == "table" or child.find("td"):
            rows = child.select("tr")
            for i, row in enumerate(rows):
                cells = [
                    cell.get_text(strip=True, separator=" ")
                    for cell in row.select("td")
                ]

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                if not any(cells):
                    continue

                # –ü–µ—Ä–≤—ã–π —Ä—è–¥ ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
                if i == 0 and len(cells) >= 2:
                    code = cells[0]
                    desc = cells[1]
                    temp_table_lines.append(f"üìò {code}\n{desc}")

                # –û–±—ã—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏: –ø–æ–¥–ø–æ–∑–∏—Ü–∏–∏
                elif len(cells) == 2:
                    temp_table_lines.append(f"‚Ä¢ {cells[0]} {cells[1]}")
                elif len(cells) == 3:
                    temp_table_lines.append(f"‚Ä¢ {cells[1]} {cells[2]}")
                elif len(cells) == 1:
                    temp_table_lines.append(f"{cells[0]}")

        # –ü–∞—Ä–∞–≥—Ä–∞—Ñ
        elif child.name == "p":
            text = child.get_text(strip=True, separator=" ")
            if text:
                # –µ—Å–ª–∏ –±—ã–ª–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω—ã —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã ‚Äî –¥–æ–±–∞–≤–∏–º –∏—Ö –±–ª–æ–∫–æ–º
                if temp_table_lines:
                    result_parts.append("\n".join(temp_table_lines))
                    temp_table_lines = []
                result_parts.append(text)

    # –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –±—ã–ª–∞ –≤ –∫–æ–Ω—Ü–µ
    if temp_table_lines:
        result_parts.append("\n".join(temp_table_lines))

    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å—ë –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ª–∏–º–∏—Ç—É
    full_text = "\n\n".join(result_parts)
    pages = textwrap.wrap(
        full_text, width=4000, break_long_words=False, replace_whitespace=False
    )

    return pages or ["–ü–æ—è—Å–Ω–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"]


async def fetch_examples(hs_code):
    url = f"https://www.ifcg.ru/kb/tnved/{hs_code}/"
    async with aiohttp.ClientSession() as session:
        html = await fetch_html(session, url)

    soup = BeautifulSoup(html, "html.parser")

    examples = []
    for sample in soup.select(".row.row-in.tnv-samples"):
        example_text = sample.select_one(".col-md-8")
        if example_text:
            text = example_text.get_text(strip=True).replace("\xa0", " ")
            examples.append(f"‚Ä¢ {text}")

    if not examples:
        return ["–ü—Ä–∏–º–µ—Ä—ã –¥–µ–∫–ª–∞—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç"]

    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤, —Å–∫–æ–ª—å–∫–æ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –æ–¥–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ Telegram
    current_text = "üìù <b>–ü—Ä–∏–º–µ—Ä—ã –¥–µ–∫–ª–∞—Ä–∏—Ä–æ–≤–∞–Ω–∏—è:</b>\n\n"
    for example in examples:
        if len(current_text) + len(example) + 2 > 4000:
            break
        current_text += example + "\n\n"

    return current_text.strip()


async def parse_tks_info(hs_code: str):
    content = await fetch_tks_info(hs_code)

    soup = BeautifulSoup(content, "html.parser")

    # –ü–∞—Ä—Å–∏–º —Å–µ–∫—Ü–∏–∏ –ò–º–ø–æ—Ä—Ç –∏ –≠–∫—Å–ø–æ—Ä—Ç
    info_sections = soup.select(".product-info__section")
    result = ""

    for section in info_sections:
        section_title = section.select_one(".product-info__section-title").text.strip()
        result += f"\n*{section_title}*\n\n"

        for row in section.select("table.product-info__table tr"):
            cols = row.find_all("td")
            if len(cols) >= 2:
                label = cols[0].text.strip()
                value = cols[1].text.strip()
                result += f"*{label}* {value}\n"

    return result


async def fetch_tks_info(
    hs_code: str, max_retries: int = 3, base_delay: float = 2.0
) -> str:
    """
    –î–æ—Å—Ç–∞—ë–º HTML –±–ª–æ–∫–∞ —Å –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—è–º–∏ –ø–æ –∫–æ–¥—É —á–µ—Ä–µ–∑ AJAX POST,
    —Å —Ä–µ—Ñ–æ—Ä–æ–º –∏ CSRF-—Ç–æ–∫–µ–Ω–æ–º –∏ —Å retry –ø—Ä–∏ —Å–±–æ—è—Ö.
    """
    base_url = "https://www.tks.ru/db/tnved/tree/"
    info_url = base_url + "info/"

    timeout = aiohttp.ClientTimeout(total=30)
    for attempt in range(1, max_retries + 1):
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                # 1) GET, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å csrftoken –≤ –∫—É–∫–∞—Ö
                async with session.get(base_url) as resp_get:
                    resp_get.raise_for_status()
                    # –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω –∏–∑ –∫—É–∫–∏
                    csrf_token = session.cookie_jar.filter_cookies(base_url)[
                        "csrftoken"
                    ].value

                # 2) –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∏ form-data
                headers = {
                    "Referer": base_url,
                    "X-CSRFToken": csrf_token,
                    "X-Requested-With": "XMLHttpRequest",
                    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
                }
                data = {
                    "code": hs_code,
                    "csrfmiddlewaretoken": csrf_token,
                }

                # 3) POST –∑–∞–ø—Ä–æ—Å–∞
                async with session.post(
                    info_url, data=data, headers=headers
                ) as resp_post:
                    resp_post.raise_for_status()  # –±—Ä–æ—Å–∏—Ç ClientResponseError, –µ—Å–ª–∏ –∫–æ–¥ !=200
                    result = await resp_post.text()
                    return result

        except (aiohttp.ClientConnectorError, aiohttp.ClientResponseError) as e:
            # —Å–µ—Ç–µ–≤—ã–µ –æ—à–∏–±–∫–∏ –∏–ª–∏ HTTP != 2xx
            if attempt == max_retries:
                raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ: {e}") from e
            # –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –º–æ–∂–Ω–æ —Ç–∞–∫:
            print(f"[TKS] –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
        except Exception as e:
            # –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏
            if attempt == max_retries:
                raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ TKS: {e}") from e
            print(f"[TKS] –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞, retry {attempt}: {e}")

        # –æ–∂–∏–¥–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π: —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –±—ç–∫–æ—Ñ—Ñ + jitter
        delay = base_delay * (2 ** (attempt - 1)) + random.uniform(0, 0.5)
        await asyncio.sleep(delay)


async def parse_tks_info2(browser, hs_code: str) -> str:
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ DNS ‚Äî —á—Ç–æ–±—ã –Ω–µ –∂–¥–∞—Ç—å —Ç–∞–π–º–∞—É—Ç Playwright, –µ—Å–ª–∏ –¥–æ–º–µ–Ω –Ω–µ —Ä–µ–∑–æ–ª–≤–∏—Ç—Å—è
    try:
        socket.gethostbyname("www.tks.ru")
    except socket.gaierror:
        return "‚ö†Ô∏è –°–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

    max_retries = 5
    backoff = 2.0  # —Å–µ–∫—É–Ω–¥—ã

    for attempt in range(1, max_retries + 1):
        context = await browser.new_context()
        page = await context.new_page()
        page.set_default_navigation_timeout(15_000)
        page.set_default_timeout(10_000)
        try:
            await page.goto(
                "https://www.tks.ru/db/tnved/tree/", wait_until="domcontentloaded"
            )
            await page.fill("#tnved-search__input", hs_code)
            await page.click("#tnved-search__submit")
            await page.wait_for_selector(".tree-list__code", timeout=10_000)
            await page.click(f".tree-list__code:text('{hs_code}')")
            await page.wait_for_selector("#code_info", timeout=10_000)
            content = await page.content()
            # 6. –ü–∞—Ä—Å–∏–º BeautifulSoup‚Äô–æ–º
            soup = BeautifulSoup(content, "html.parser")
            info_sections = soup.select("#code_info .product-info__section")
            if not info_sections:
                return "‚ÑπÔ∏è –ü–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∫–æ–¥—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."

            result = ""
            for section in info_sections:
                title = section.select_one(".product-info__section-title").text.strip()
                result += f"\n*{title}*\n"
                for row in section.select("table.product-info__table tr"):
                    cols = row.find_all("td")
                    if len(cols) >= 2:
                        label = cols[0].text.strip()
                        value = cols[1].text.strip()
                        result += f"‚Ä¢ *{label}*: {value}\n"
            return result

        except PlaywrightTimeoutError:
            if attempt == max_retries:
                return "‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö."
        except Exception as e:
            if attempt == max_retries:
                return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: {e}"
        finally:
            await page.close()
            await context.close()

        await asyncio.sleep(backoff + random.random() * 0.5)
        backoff *= 2

    # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
    return "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."


# asyncio.run(parse_ifcg("–®–∏–Ω—ã+–õ–µ–≥–∫–æ–≤—ã–µ+–†–µ–∑–∏–Ω–∞"))
